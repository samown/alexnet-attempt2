{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59c5482",
   "metadata": {},
   "source": [
    "# AlexNet Training Pipeline - iFood 2019\n",
    "\n",
    "Train 4 AlexNet variants (Baseline, Modified1, Modified2, Combined) sequentially on iFood dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176a1b3",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc597f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path.cwd().parent / 'src' / 'config' / 'config.yaml'\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  - Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  - Number of classes: {config['data']['num_classes']}\")\n",
    "print(f\"  - Image size: {config['data']['image_size']}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf802e6",
   "metadata": {},
   "source": [
    "## Import Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import IFoodDataset\n",
    "from data.transforms import get_transforms\n",
    "from models.alexnet_baseline import AlexNetBaseline\n",
    "from models.alexnet_modified1 import AlexNetModified1\n",
    "from models.alexnet_modified2 import AlexNetModified2\n",
    "from models.alexnet_combined import AlexNetCombined\n",
    "from training.train import train_epoch, validate\n",
    "from training.utils import save_checkpoint\n",
    "\n",
    "print(\"‚úì Custom modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419e04c",
   "metadata": {},
   "source": [
    "## Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "checkpoints_dir = Path.cwd().parent / 'checkpoints'\n",
    "plots_dir = Path.cwd().parent / 'results' / 'plots'\n",
    "logs_dir = Path.cwd().parent / 'results' / 'logs'\n",
    "\n",
    "for directory in [checkpoints_dir, plots_dir, logs_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Checkpoints dir: {checkpoints_dir}\")\n",
    "print(f\"‚úì Plots dir: {plots_dir}\")\n",
    "print(f\"‚úì Logs dir: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb09a2",
   "metadata": {},
   "source": [
    "## Check Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data structure\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Data dir exists: {data_dir.exists()}\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(f\"\\nContents of data directory:\")\n",
    "    for item in sorted(data_dir.iterdir()):\n",
    "        if item.is_dir():\n",
    "            print(f\"  üìÅ {item.name}/\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b6818",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transforms\n",
    "train_transform = get_transforms(split='train', config=config)\n",
    "val_transform = get_transforms(split='val', config=config)\n",
    "\n",
    "# Create datasets\n",
    "train_csv = data_dir / 'annotations' / 'train_labels.csv'\n",
    "val_csv = data_dir / 'annotations' / 'val_labels.csv'\n",
    "train_img_dir = data_dir / 'train_set'\n",
    "val_img_dir = data_dir / 'val_set'\n",
    "\n",
    "print(f\"Training CSV exists: {train_csv.exists()}\")\n",
    "print(f\"Validation CSV exists: {val_csv.exists()}\")\n",
    "\n",
    "if train_csv.exists() and train_img_dir.exists():\n",
    "    train_dataset = IFoodDataset(\n",
    "        csv_file=str(train_csv),\n",
    "        root_dir=str(train_img_dir),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    print(f\"‚úì Training dataset: {len(train_dataset)} samples\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training data not found.\")\n",
    "    train_dataset = None\n",
    "\n",
    "if val_csv.exists() and val_img_dir.exists():\n",
    "    val_dataset = IFoodDataset(\n",
    "        csv_file=str(val_csv),\n",
    "        root_dir=str(val_img_dir),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    print(f\"‚úì Validation dataset: {len(val_dataset)} samples\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Validation data not found.\")\n",
    "    val_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "if train_dataset and val_dataset:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['training']['num_workers'],\n",
    "        pin_memory=config['training']['pin_memory']\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['evaluation']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['evaluation']['num_workers'],\n",
    "        pin_memory=config['training']['pin_memory']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Training loader: {len(train_loader)} batches\")\n",
    "    print(f\"‚úì Validation loader: {len(val_loader)} batches\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39db22d",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c25090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, model_name, checkpoint_dir, num_epochs=10):\n",
    "    \"\"\"Train a single model\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        momentum=config['training']['momentum'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config['training']['scheduler']['step_size'],\n",
    "        gamma=config['training']['scheduler']['gamma']\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, epoch, use_wandb=False)\n",
    "        history['train_loss'].append(train_metrics['train_loss'])\n",
    "        history['train_acc'].append(train_metrics['train_accuracy'])\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device, epoch, use_wandb=False)\n",
    "        history['val_loss'].append(val_metrics['val_loss'])\n",
    "        history['val_acc'].append(val_metrics['val_accuracy'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  ‚úì Checkpoint: {checkpoint_path.name}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = checkpoint_dir / 'final_model.pt'\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    print(f\"‚úì Final model saved: {final_path.name}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3afb5",
   "metadata": {},
   "source": [
    "## Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37da875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = {\n",
    "    'Model_A': {\n",
    "        'name': 'alexnet_baseline',\n",
    "        'class': AlexNetBaseline,\n",
    "        'checkpoint_dir': checkpoints_dir / 'model_a',\n",
    "        'config': {'num_classes': config['data']['num_classes'], 'dropout': 0.5}\n",
    "    },\n",
    "    'Model_B': {\n",
    "        'name': 'alexnet_modified1',\n",
    "        'class': AlexNetModified1,\n",
    "        'checkpoint_dir': checkpoints_dir / 'model_b',\n",
    "        'config': {'num_classes': config['data']['num_classes'], 'dropout': 0.5, 'use_batch_norm': True}\n",
    "    },\n",
    "    'Model_C': {\n",
    "        'name': 'alexnet_modified2',\n",
    "        'class': AlexNetModified2,\n",
    "        'checkpoint_dir': checkpoints_dir / 'model_c',\n",
    "        'config': {'num_classes': config['data']['num_classes'], 'dropout': 0.5, 'use_leaky_relu': True}\n",
    "    },\n",
    "    'Model_D': {\n",
    "        'name': 'alexnet_combined',\n",
    "        'class': AlexNetCombined,\n",
    "        'checkpoint_dir': checkpoints_dir / 'model_d',\n",
    "        'config': {'num_classes': config['data']['num_classes'], 'dropout': 0.5, 'use_batch_norm': True, 'use_leaky_relu': True}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create checkpoint directories\n",
    "for model_config in models_config.values():\n",
    "    model_config['checkpoint_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Models configured:\")\n",
    "for model_name, model_config in models_config.items():\n",
    "    print(f\"  ‚úì {model_name}: {model_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c15b7",
   "metadata": {},
   "source": [
    "## Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset and val_dataset:\n",
    "    all_histories = {}\n",
    "    \n",
    "    for model_key, model_config in models_config.items():\n",
    "        # Create model\n",
    "        model = model_config['class'](**model_config['config'])\n",
    "        \n",
    "        # Train\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            model_name=model_key,\n",
    "            checkpoint_dir=model_config['checkpoint_dir'],\n",
    "            num_epochs=10\n",
    "        )\n",
    "        \n",
    "        all_histories[model_key] = history\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"‚úì All models trained successfully!\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot train - data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe202d",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset and val_dataset:\n",
    "    # Save histories as JSON\n",
    "    histories_json = {}\n",
    "    for model_name, history in all_histories.items():\n",
    "        histories_json[model_name] = {\n",
    "            'train_loss': history['train_loss'],\n",
    "            'train_acc': history['train_acc'],\n",
    "            'val_loss': history['val_loss'],\n",
    "            'val_acc': history['val_acc']\n",
    "        }\n",
    "    \n",
    "    histories_path = logs_dir / 'training_histories.json'\n",
    "    with open(histories_path, 'w') as f:\n",
    "        json.dump(histories_json, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Histories saved: {histories_path}\")\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(f\"{'Model':<15} {'Train Loss':<15} {'Train Acc':<15} {'Val Loss':<15} {'Val Acc':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    for model_name, history in all_histories.items():\n",
    "        print(f\"{model_name:<15} {history['train_loss'][-1]:<15.4f} {history['train_acc'][-1]:<15.2f} {history['val_loss'][-1]:<15.4f} {history['val_acc'][-1]:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5db774",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset and val_dataset:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('AlexNet Models Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Training loss\n",
    "    ax = axes[0, 0]\n",
    "    for model_name, history in all_histories.items():\n",
    "        ax.plot(history['train_loss'], label=model_name, marker='o', markersize=3)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    ax = axes[0, 1]\n",
    "    for model_name, history in all_histories.items():\n",
    "        ax.plot(history['val_loss'], label=model_name, marker='o', markersize=3)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training accuracy\n",
    "    ax = axes[1, 0]\n",
    "    for model_name, history in all_histories.items():\n",
    "        ax.plot(history['train_acc'], label=model_name, marker='o', markersize=3)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Training Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    ax = axes[1, 1]\n",
    "    for model_name, history in all_histories.items():\n",
    "        ax.plot(history['val_acc'], label=model_name, marker='o', markersize=3)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Validation Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = plots_dir / 'training_curves.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úì Plot saved: {plot_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24d544",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Checkpoints: {checkpoints_dir}\")\n",
    "print(f\"‚úì Plots: {plots_dir}\")\n",
    "print(f\"‚úì Results: {logs_dir}\")\n",
    "print(\"\\nNext:\")\n",
    "print(\"1. Download results from RunPod\")\n",
    "print(\"2. Analyze performance\")\n",
    "print(\"3. Write final report\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
